Matthieu FARANDJIS
INF3-FI

<div align="center">
<img height="95" width="400" src="https://www.uvsq.fr/medias/photo/iut-velizy-villacoublay-logo-2020-ecran_1580904185110-jpg?ID_FICHE=214049" title="logo uvsq v√©lizy"/>
<h1>Programmation Avanc√©e & Qualit√© de d√©veloppement</h1>
<h2>- Rapport n¬∞2 -</h2>
</div>


### Rappel d√©finitions : <br>


<br><br>
### Liens int√©ressants :

**Sur  :** https://<br>

<br><br>
## Plan

- **I - M√©thode de Monte-Carlo**
- **II - Algorithme et parall√©lisation**
  - a) It√©ration parall√®le
  - b) Master-Worker
- **III - Mise en ≈ìuvre sur machine √† m√©moire partag√©e**
  - a) Analyse d'Assigment102
  - b) Analyse Pi.java
- **IV - Qualit√© et test de performance (cf R05.08 Q Dev)**
- **V - Mise en ≈ìuvre en m√©moire distribu√©e**
  - a) (Analyse) JavaSocket
  - b) MasterWorker
- **VI - Test de performance Master-Worker distribu√©e**
<br><br><br>


## I - M√©thode de Monte-Carlo

La m√©thode de Monte-Carlo permet de pr√©dire un r√©sultat √† partir d'un ensemble de valeurs g√©n√©r√©es al√©atoirement.<br>
Elle effectue plusieurs it√©rations pour recalculer et affiner le r√©sultat.<br>
L'un des principes fondamentaux de la m√©thode est qu'une augmentation du nombre d'it√©rations am√©liore la pr√©cision de l'estimation.<br>
<br>
*M√©thode d'apr√®s IBM : https://www.ibm.com/fr-fr/topics/monte-carlo-simulation* <br>
<br><br>
**Calcul de œÄ par une m√©thode de MonteCarlo, d'apr√®s le TP4 fait en cours**<br>
Soit l'aire $`A_{\tfrac{1}{4}d}`$ d'un quart de disque de rayon $`r = 1`$.<br>

$$A_{\tfrac{1}{4}d} =  \frac{\pi r^2}{4} = \frac{\pi}{4}$$


<br><img src="img/figure1.png" width="300"/><br>
_**Figure 1 :** Tirage al√©atoire dans un carr√© de c√¥t√© r = 1._


Soit l'aire d'un carr√© de c√¥t√© $`r=1`$, $`A_{c} = r^2 = 1`$ <br>
<br>
Soient les points $`X_{p}(x_{p},y_{p})`$ dont les coordonn√©es sont tir√©es selon une loi $`U (]0,1[)`$.<br>
La probabilit√© que $`X_{p}`$ soit tir√© dans le quart de disque est

$$P = \frac{A_\frac{{1}{4}d}{A_{c}}} = \frac{\pi}{4}$$

On effectue $`n_{tot}`$ tirages al√©atoires.<br>
Soit $`n_{cible}`$ le nombre de points tir√©s dans le quart de disque.<br>
<br>
Si $`n_{tot}`$ est rand, alors on peut approximer P pour $`P = \frac{n_{cible}}{n_{tot}} \approx \frac{\pi}{4}`$<br>
<br>
D'o√π $`\pi \approx 4 \times \frac{n_{cible}}{n_{tot}}`$<br>
<br><br>

**ALGORITHME : Impl√©mentation s√©quentielle de la m√©thode de Monte-Carlo**<br>

<img src="img/algo_mc.png" width="500"/>


- On identifie deux
  - T0 : Tirer et compter $`n_{tot}`$ points
  - T1 : Calculer $`\pi`$
- T0 se d√©compose en $`n_{tot}`$ sous t√¢ches
  - $`T_{0P1}`$ : tirer $`X_{p}`$
  - $`T_{0P2}`$ : incr√©menter $`n_{cible}`$
- *D√©pendance entre t√¢ches :*
  - T1 d√©pend de T0
  - $`T_{0P2}`$ d√©pend de $`T_{0P1}`$
  - Les $`T0P1`$ sont ind√©pendats selon p
  - Les $`T0P2`$ sont ind√©pendats selon p


<br><br><br>
## II - Algorithme et parall√©lisation

### a) It√©ration parall√®le

L'it√©ration parall√®le est aussi appel√©e parall√©lisme de boucle et parall√©lisme it√©ratif.<br>
Dans un algorithme parall√®le, on suppose que le calcul effectu√© par une unit√© de calcul est ind√©pendant de celui effectu√© par une autre unit√© de calcul.<br>
L'adjectif "parall√®le" attribu√© √† un tel algorithme provient du fait que cela correspond √† une architecture de type SIMD (une Seule Instruction, Plusieurs Donn√©es).<br>
*Source :*
  - *Explications adapt√©s pour Assigment102 :*<br>
    https://dpt-info.u-strasbg.fr/~cronse/TIDOC/ALGO/parseq.html


<br>
L'algorithme √† it√©ration parall√®le de la m√©thode de Monte-Carlo que nous avons √©tudi√© est Assignment102.<br>
<br>

Voici un extrait du code d'Assigment102 :<br>
```
	class MonteCarlo implements Runnable {
		@Override
		public void run() {
			double x = Math.random(); // g√©n√©ration d'un nombre al√©atoire, donc avec y => g√©n√©ration d'un point
			double y = Math.random(); // Si on pr√©cise une graine (seed) : le tirage est d√©terministe. Utile si on veut retester le code (donc en s'assurant qu'on obtient le m√™me r√©sultat)
			if (x * x + y * y <= 1)
				nAtomSuccess.incrementAndGet(); // On remarque que √ßa resemble √† notre pseudo code avec machin++. C'est un msg envoy√© √† nAtomSuccess
		}
	}

	public double getPi(int numWorkers) {
		int nProcessors = numWorkers; // runtime : c'est l'environnement pendant le temps d'ex√©cution du code. Ici elle nous propose de regarder le nb processeurs dispo
		ExecutorService executor = Executors.newWorkStealingPool(nProcessors); // il fixe le nb de thread au nb de processeur d√©tect√© par la JVM (donc si dans le bios on a autoris√© l'hyperthreading, alors il va dire que c'est 8 professeurs (8 coeurs en r√©alit√©s), sinon 4)
		for (int i = 1; i <= nThrows; i++) {
			Runnable worker = new MonteCarlo();
			executor.execute(worker);
		}
		executor.shutdown();
		while (!executor.isTerminated()) {
		}
		value = 4.0 * nAtomSuccess.get() / nThrows;
		return value;
	}
```

### b) Master-Worker

Le premier algorithme Master-Worker de la m√©thode de Monte-Carlo que nous avons √©tudi√© est Pi.java.<br>
Master cr√©e puis lance des t√¢ches qui, via le paradigme des futures, font une r√©solution de d√©pendance.<br>
<br>
Pi.java est plus efficace que Assignment102, puisque si l'on cr√©e 500 000 t√¢ches, l'OS g√®re lui-m√™me celles-ci, ce qui prend plus de temps que de faire :
g√©n√©ration nombre al√©atoire x, g√©n√©ration nombre al√©atoire y, test, incr√©ment (~40 cycles).<br>
<br><br>
Il est difficile de montrer un exemple du code calculant Monte-Carlo, mais nous pouvons le visualiser via le code disponible sur ce d√©p√¥t.<br>
<br><br>


<br><br><br>

## III - Mise en ≈ìuvre sur machine √† m√©moire partag√©e

### a) Analyse d'Assigment102

Le code de Assignment102 calcule une valeur approximative de ùúã √† partir de la m√©thode de Monte-Carlo.<br>
On y retrouve deux d√©pendances nouvelles : AtomicInteger et Executor.<br>

- **Atomic Integer :**<br>
  Encapsule une valeur enti√®re qui peut √™tre mise √† jour de mani√®re atomique.<br>
  C'est-√†-dire que toutes les op√©rations de lecture-modification-√©criture sur cette valeur sont effectu√©es comme une seule unit√© ins√©cable. Cela garantit la s√©curit√© des threads (thread-safety) sans n√©cessiter de synchronisation explicite.<br>
  Un AtomicInteger est utilis√© dans les applications telles que les compteurs incr√©ment√©s atomiquement et ne peut pas √™tre utilis√© comme remplacement d‚Äôun java.lang.Integer.<br>
  <br>
  *Sources :*
  - *https://www.jmdoudoux.fr/java/dej/chap-acces_concurrents.htm*
  - *https://learn.microsoft.com/fr-fr/dotnet/api/java.util.concurrent.atomic.atomicinteger?view=net-android-34.0*
  - *ChatGPT (la phrase du "C'est-√†-dire")*
    <br><br><br>
- **Executor :**<br>
  L'interface java.util.concurrent.Executor d√©crit les fonctionnalit√©s permettant l'ex√©cution diff√©r√©e de t√¢ches impl√©ment√©es sous la forme de Runnable.<br>
  C‚Äôest un support pour les Threads en Java √† un plus haut niveau que la classe Thread.<br>
  Il permet de d√©coupler la soumission des t√¢ches de la m√©canique des Threads (Execution, Ordonnancement)<br>
  On peut ici g√©rer des pool de Threads. Chaque Thread du pool peut √™tre r√©utilis√© dans un Executor.<br>
  L‚Äôinterface Executor d√©finit la m√©thode execute.<br>
  <br>
  *Sources :*
  - *https://jmdoudoux.developpez.com/cours/developpons/java/chap-executor.php#executor-1*
  - *Cours de M. DUFAUD : CM4-complement-parallelisation-et-Java.pdf*
- **Algorithme Workstealing (abord√© en cours) :**<br>
  Le principe de cet algorithme est que chaque worker poss√®de une liste de t√¢che.<br>
  Lorsqu'un worker n'en poss√®de plus, plut√¥t qu'attendre de nouvelles t√¢ches, il va en extraire chez un autre worker.<br>
  <br>
  De telle mani√®re, non seulement chaque worker √† peu peut pr√®s le m√™me nombre de t√¢ches, mais aussi, on s'assure que les workers soient toujours actif.<br>
  On garanti donc l'optimisation du fonctionnement du programme.<br>
  *Sources :*
  - https://www.linkedin.com/advice/0/what-benefits-drawbacks-using-work-stealing?lang=fr&originalSubdomain=fr
- **Les futures :**<br>
  Paradigme de programmation qui est tr√®s pratique, qui permet de d√©finir quand on cr√©er notre t√¢che le sch√©ma de d√©pendance entre les t√¢ches.<br>
  L'id√©e est de dire que la t√¢che va envoyer un r√©sultat dans le futur; On ne sait pas quand il va l'envoyer, mais plus loin dans le code on va utliser son code.<br>
  On dit que la t√¢che x va envoyer un res y dans le temps. la t√¢che s'√©xecute, on continu le code (d'autres trucs s'ex√©cutent), puis un moment x renvoi le resultat, et on fait un calcul.<br>
  - le code √† ce moment l√†, pour calculer doit attendre le resultat de x, c'est un point de synchronisation

<br><br>
On d√©finit un objet executor qui est un support de thread.<br>
On va lui dire : tu d√©finie le nombre de threads que tu vas utiliser.<br>
-> il permet de faire l'association entre des t√¢ches et des threads de mani√®re dynamique en fonction de l'ex√©cution des t√¢ches en cours.<br>
-> Il est donc possible d'associer 1000 t√¢ches √† 4 threads (au lieu de 1000 t√¢ches = 1000 threads dans le TP mobile, vu qu'on ne pouvait pas utiliser Executor).<br>
<br>
===> On utilise d√©sormais la classe Executor, plut√¥t que la classe Thread.<br>
Le fait est que les threads en Java pr√©sentent de nombreux inconv√©nients essentiellement li√©s au fait que cette classe est de bas niveau.<br>
De ce fait, elle ne propose pas des fonctionnalit√©s tel qu'obtenir un r√©sultat d'ex√©cution d'un thread ou encore attendre la fin d'un ensemble de threads.<br>
<br>
*Sources :*<br>
*https://jmdoudoux.developpez.com/cours/developpons/java/chap-executor.php#executor-1*

<br><br>
**Note :**
- start et run sont deux choses diff√©rentes
- V est un type g√©n√©rique
- overhead : ordonnanceur de l'OS

<br><br>
**Quelques annotations sur le code :**<br>
- **double y = Math.random();**
  - Si on pr√©cise une graine (seed) : le tirage est d√©terministe. Utile si on veut retester le code (donc en s'assurant qu'on obtient le m√™me r√©sultat)
- **int nProcessors = Runtime.getRuntime().availableProcessors();**
  - runtime : c'est l'environnement pendant le temps d'ex√©cution du code. Ici elle nous propose de regarder le nb processeurs dispo
- **Executors.newWorkStealingPool(nProcessors);**
  - il fixe le nb de thread au nb de processeur d√©tect√© par la JVM (donc si dans le bios on a autoris√© l'hyperthreading, alors il va dire que c'est 8 professeurs (8 coeurs en r√©alit√©s), sinon 4)
<br><br>

Voici un diagramme UML d'Assigment102 :<br>
<img src="img/uml_assigment102.png" width="600"/><br>

*Les outils suivants m'ont aid√© : StarUML, ChatGPT*<br>

### b) Analyse Pi.java


**Paradigme de Pi.java :** Master Worker

On a un Master qui va cr√©er puis lancer des t√¢ches qui via le paradigme des futures fait une r√©solution de d√©pendance.<br>
<br><br>

Voici un sch√©ma montrant l'interraction entre 1 Master et 3 Workers : <br>
<img src="img\schema_dufaud_1M_3W_1M.png" width="600"/><br>

*Les outils suivants m'ont aid√© : Excalidraw*<br>


<br><br>
Pi.java est plus efficace que Assigmnent102, puisque si on cr√©er 500000 t√¢che, l'OS g√®re lui-m√™me ce qui prend plus de temps que de faire :
g√©n√©ration nb al√©atoire x, g√©n√©ration nb al√©atoire y, test, incr√©ment (~40 cycles).<br>
<br><br>
**Quelques annotations sur le code :**<br>
- **Executors.newFixedThreadPool(numWorkers)**
  - Renvoyer une instance de type ExecutorService qui utilise un pool de threads dont la taille est fixe. Les t√¢ches √† ex√©cuter sont stock√©es dans une queue
    <br><br>

Voici un diagramme UML de Pi.java :<br>
<img src="img/uml_pi.java.png" width="600"/><br>

*Les outils suivants m'ont aid√© : StarUML, ChatGPT*<br>



<br><br><br>

## IV - Qualit√© et test de performance (cf R05.08 Q Dev)

### a) Mise en place

De prime abord, Assigment102 et Pi.java affiche les r√©sultarts de mani√®re diff√©rente. Afin de mieux comparer et √©tudiier leurs efficacit√©s, nous avons du uniformiser les sorties.<br>
C'est √† dire s'assurer que chaque code affiche/renvoie la m√™me chose.<br>
<br><br>
Ainsi les prints ci-dessous sont dans le m√™me format que ce soit dans Assigment102 ou Pi.java.<br>

<img src="img\uni_sorties.png" width="1000"/><br>
**Code de Pi.java**

<br><br>
Pour traiter ces informations et facilit√© l'automatisation du lancement de plusieurs fois d'affiler le code, nous allons sauvegarder ces informations dans un document.<br>
Pour cela on va cr√©er une classe WriteToFile  :<br>
<img src="img\wtf.png" width="600"/>
<br><br>
Comme nous pouvons le voir, on cr√©er en premier un nom √† notre fichier.<br>
Nous indiquons le jour du test, ainsi que la machine sur lequel on teste (gr√¢ce √† `InetAddress.getLocalHost()`), sachant que le nom de chaque machine de l'IUT correspond √† la salle et √† sa position dans la salle.<br>
C'est indispensable car les r√©sultats peuvent vari√©s d'une machine √† l'autre.<br>
Pour de meilleurs r√©sultats, dans les faits, il faudrait m√™me fermer tous les logiciels ouvert, voir m√™me utiliser le terminal plut√¥t qu'IntelliJ et dans l'id√©al, ne pas d'utiliser l'interface graphique Windows.<br>
<br>
Ensuite nous √©crivons √† la suite du fichier les donn√©es re√ßu, on sauvegarde et on affiche si tout s'est bien pass√© ou non.<br>

<br><br>
Enfin, nous allons traiter ces fichiers via un programme Python afin d'√©tablir un graphique et √©tudier les r√©sultats.<br>
Plus pr√©cis√©ment, nous allons √©tudier la scalabilit√© forte et faible d'Assignment102 et Pi.java sur les crit√®res de temps d'ex√©cution et de speed-up.<br>

### b) D√©finitions des termes
- **Speed-up :**<br>
  Le speed-up, not√© Sp, est le gain de vitesse d‚Äôex√©cution en fonction du nombre de processus P.<br>
  L'id√©e est donc de mesurer le gain de performance obtenu en ex√©cutant une t√¢che sur plusieurs processeurs (ou c≈ìurs) par rapport √† un seul processeur.
- **Temps d'ex√©cution**<br>
  Le temps d'ex√©cution correspond au temps que demande le programme pour effectuer le calcul de pi.<br>
  Dans le cas du paradigme Master-Worker, le temps d'ex√©cution correspond au temps des √©changes entre le Master et les Workers + le temps que demande les Workers pour calculer + le temps d'assembler un r√©sultat final par Master.
- **Scalabilit√© forte :**<br>
  La scalabilit√© forte consiste √©tudier ce qu'il se passe lorsque l'on ajoute des processus pour un probl√®me de taille fixe.
- **Scalabilit√© faible :**<br>
  La scalabilit√© faible consiste √©tudier ce qu'il se passe lorsqu l'on augmente simultan√©ment la taille du probl√®me et le nombre de processus.

### c) Analyse
L'objectif de cette √©tude est de prouver quel est le meilleur paradigme pour calculer Pi √† l'aide de la m√©thode de Monte-Carlo entre Assignment102 et Pi.java.<br>
<br>
Pour rappel :
- Paradigme d'Assignment102 : It√©ration parall√®le
- Paradigme de Pi.java : Master-Worker

<br><br>

Les tests suivant ont √©t√© effectu√©s sur mon ordinateur personnel, voici sa configuration :

- Processeur : Intel Core i5-9400F, 2.90Ghz, 6 Coeurs, 1 Socket
- RAM : 8Go
- Windows 10 Home 22H2, build : 19045.5247
- Carte Graphique : NVIDIA GeForce FTX 1050
- Attention, certains logiciel fonctionnaient en fond, par ailleurs l'interface graphique de Windows √©tait d√©marr√©. Cela influe sur les performances de l'ordinateur.

R√©sultats obtenus :
![](img\etude_sca.png)

- **Exp√©rience n¬∞1 en Scalabilit√© Forte : Est-ce que le temps d‚Äôex√©cution diminue lorsque j‚Äôajoute des processus pour un probl√®me de taille fixe ?**
  ![](img\etude_sca_e1.png)
  Nous remarquons que contrairement √† Assignment102, pour Pi.java, le temps d'ex√©cution diminue.<br>
  Cela s'explique par le fait que l'on divise la taille du probl√®me entre les processus. Vu que les processus traite des probl√®mes de plus en plus petit, c'est tr√®s rapide bien qu'ils soient nombreux.<br>
  En revanche, pour Assignment102, nous donnons un probl√®me de m√™me taille √† chaque processus, donc √ßa ne peut qu'augmenter. Cependant nous remarquons que √ßa augmente de plus en plus lentement. <br>
  <br>
  Pi.java est donc le gagnant de cette exp√©rience.


- **Exp√©rience n¬∞2 en Scalabilit√© Forte : Comment les ressources sont utilis√©s lorsque j'ajoute des processus pour un probl√®me de taille fixe ?**
  ![](img\etude_sca_e2.png)

  La courbe bleu correspond au speed-up id√©al en fonction du nombre de processus.<br>
  Cela signifie que dans l'id√©al, si la m√©thode est parfaitement parall√®le, le fait de doubler le nombre de processus divise par deux le temps de calcul. En d'autre terme : il y a une excellente r√©partiion des ressources.<br>
  <br>
  - **Pi.java**<br>
    Nous remarquons que le speedup suit le speed-up id√©al au d√©but lorsque l'on utilise 1 √† 4 processus, puis augmente tr√®s lentement de 4 √† 16 processus avant de stagner.<br>
    Cela signifie que Pi.java est efficace lorsque l'on utilise un petit nombre de processus.<br>
    Apr√®s cela, on peut d√©duire que l'interaction de plus en plus importante entre le Master et les Workers fait perdre de l'efficacit√© au programme. Ce temps de traitement de plusieurs processus par la machine s'appel "overhead", nous en parlons en partie V a).
  - **Assignment102**<br>
    C'est la catastrophe ! M√™me avec plus de processus, le r√©sultat est mauvais : cela tend vers 0.<br>
    Cela ressemble √† une courbe de scalabilit√© faible (voir exp√©rience n¬∞4), mais c'est bien le r√©sultat de notre exp√©rience en scalabil√© forte.<br>
    On en d√©duit une tr√®s mauvaise r√©partition de la charge de travail entre les processus, chose que nous avons d√©j√† remarqu√© dans l'exp√©rience n¬∞1.
  
  <br>
  Par cons√©quent, Pi.java est de nouveau vainqueur : cet algorithme est plus efficace qu'Assigment102.


- **Exp√©rience n¬∞3 en Scalabilit√© Faible : Est-ce que le temps d‚Äôex√©cution diminue lorsque j‚Äôaugmente simultan√©ment la taille du probl√®me et le nombre de processus ?**
  ![](img\etude_sca_e3.png)
  Nous remarquons que dans les deux cas, le temps d'ex√©cution augmente. Cependant, ici √©galement Pi.java reste le plus effiace : il augmente beaucoup moins vite qu'Assignment102 si l'on se fie √† l'axe des ordonn√©es.<br>
  Nous remarquons √©galement que c'est une courbe lin√©aire, ce qui est normal puisque le probl√®me grandi proportionnellement au nombre de processus.<br>
  Il est a not√© que dans le cas de Pi.java, de 1 √† 4 processus le temps est constant, √ßa augmente d√®s qu'il y a plus de 4 processus.
  <br>
  M√™me lors d'une √©tude de scalabit√© faible, Pi.java est mieux qu'Assignment102.


- **Exp√©rience n¬∞4 en Scalabilit√© Faible : Comment les ressources sont utilis√©s lorsque j‚Äôaugmente simultan√©ment la taille du probl√®me et le nombre de processus ?**
  ![](img\etude_sca_e4.png)
  Notre observation rejoins l'oberservation sur l'exp√©rience n¬∞2 : augmenter le nombre de processus ne permet pas toujours d'avoir un bon speed-up, m√™me si la taille du probl√®me augmente proportionnellement.<br>
  En effet, comme dans l'exp√©rience n¬∞2, de 1 √† 4 la courbe suit le speed-up id√©al, apr√®s cela le co√ªt des communications entre le Master et les Workers influe trop sur les performances.<br>
  De m√™me que dans l'exp√©rience n¬∞2, Assignment102 s'effond rapidement.<br>
  <br>
  Seulement, je me demande si Pi.java ne stagne pas comme Assignment102 √† partir de 128 processus lorsque l'on compare l'allure des deux courbes.
  <br>
  La conlusion reste l√† m√™men Pi.java est mieux qu'Assignment102.<br>


A l'issue de ces quatre exp√©rience, la conclusion est sans apppel : le paradigme Master-Worker offre des meilleurs r√©sultats avec la m√©thode de Monte-Carlo que le paradigme d'it√©ration parall√®le.
<br><br><br>

## V - Mise en ≈ìuvre en m√©moire distribu√©e

### a) (Analyse) JavaSocket

En Software, le **socket** est un fichier contenant des informations.<br>
Toutes nos donn√©es, toutes les cases m√©mores sont mis dans un fichier qui est transmis dans le r√©seau.<br>
<br>
Nous en venons au terme **overhead** qui est le temps de traitement de plusieurs processus par la machine.<br>
On peut le diminuer en augmentant la charge d'un processus, car traiter quelques gros processus est moins couteux que de g√©rer plusieurs petit processus.<br>
C'est pour cela que l'architecture d'une carte graphique, utile pour des calcules parall√®les n'est pas adapt√© pour faire fonctionner un syst√®me d'exploitation.<br>
Une carte graphique g√®re plusieurs petits processus contrairement √† un processeurs qui en g√®re principalement des gros. Avec un GPU, le syst√®me serait donc tr√®s lent !<br>

### b) MasterWorker

Pour le dernier projet, on utilise deux fichiers java :

- Master, qui g√®re les Workers lanc√©s sur diff√©rents ports
- Worker, qui se lance sur un port pr√©cis. Ce qui permet d'en lancer plusieurs simultan√©ments sur diff√©rents ports.<br>
  <br><br>
  Notre objectif √† terme est de lancer un Master sur un ordinateur, et utiliser les Workers qui sont sur d'autres ordinateurs.<br>
  En cons√©quence, le code est pr√©vu pour : apr√®s une petite modification on peut donner des ip personnalis√©s √† Master pour acc√©der aux autres ordinateurs.<br>
  Pour le moment, nous allons tester sur une seule machine.<br>

Voici un diagramme UML complet de Master-Worker :<br>
<img src="img/uml_mw.png" width="800"/><br>

*Les outils suivants m'ont aid√© : StarUML, ChatGPT*<br>

<br><br>

J'ai du rajouter un morceau de code pour ex√©cuter la m√©thode de Monte-Carlo ainsi que l'enregistrement dans un fichier texte comme pour Assigment102 et Pi.java.<br>
On reprend simplement le morceau de code d'Assigment102 permettant de calculer Pi que l'on place dans une fonction qu'on appel.<br>
On aurait pu directement l'int√©grer au code, seulement celui-ci serait moins maintenable, il serait plus dure de retrouver puis de remplacer le morceau.<br>
Ici on sait ce qu'il a pr√©cis√©ment besoin et ce qu'il doit renvoyer, faire.<br>
<br>
<img src="img\permettraDeCalculerPi.png" width="350"/><br>
<img src="img\whileWorker.png" width="900"/>

<br>**Pour lancer et changer de port :**
- **Lancer sur le port 25545 :**<br>
  Sur IntelliJ pour WorkerSocker :
  - Cliquer sur le bouton "3 points vertical", puis "Edit" et mettre "25545" dans le port (sous distributedMC).
- **Lancer plusieurs instances :**
  Sur IntelliJ pour WorkerSocker :
  - Cliquer sur le bouton "3 points vertical", puis dans "Config", dans "Modify options (Build and Run)", cocher Allow m... (CTRL+U) en haut de la liste.
<br><br>
Si nous ex√©cutons nos programmes sur une seule machine, la r√©partition des Worker + Master correspond au sch√©ma du III - b).<br>
Si nous l'ex√©cutons sur plusieurs machine, cela correspondrais √† √ßa :
<img src="img\schema_dufaud_1M_3W_1M.png_sur_plusieurs_pc" width="600"/><br>

## VI - Test de performance Master-Worker distribu√©e
