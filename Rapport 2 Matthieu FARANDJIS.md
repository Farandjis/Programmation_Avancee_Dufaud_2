Matthieu FARANDJIS
INF3-FI

<div align="center">
<img height="95" width="400" src="https://www.uvsq.fr/medias/photo/iut-velizy-villacoublay-logo-2020-ecran_1580904185110-jpg?ID_FICHE=214049" title="logo uvsq v√©lizy"/>
<h1>Programmation Avanc√©e & Qualit√© de d√©veloppement</h1>
<h2>- Rapport n¬∞2 -</h2>
</div>


### Rappel d√©finitions : <br>


<br><br>
### Liens int√©ressants :

**Sur  :** https://<br>

<br><br>
## Plan

- [**I - M√©thode de Monte-Carlo**](#p1)
- [**II - Algorithme et parall√©lisation**](#p2)
  - [a) It√©ration parall√®le](#p2a)
  - [b) Master-Worker](#p2b)
- [**III - Mise en ≈ìuvre sur machine √† m√©moire partag√©e**](#p3)
  - [a) Analyse d'Assigment102](#p3a)
  - [b) Analyse Pi.java](#p3b)
- [**IV - Qualit√© et test de performance (cf R05.08 Q Dev)**](#p4)
  - [a) Mise en place](#p4a)
  - [b) D√©finitions des termes](#p4b)
  - [c) Etude sur le crit√®re d'efficiency](#p4c)
  - [d) Etude sur le crit√®re d'effictiveness pour Pi.java](#p4d)
  - [e) Etude sur le crit√®re de satisfaction pour Pi.java](#p4e)
- [**V - Mise en ≈ìuvre en m√©moire distribu√©e**](#p5)
  - [a) (Analyse) JavaSocket](#p5a)
  - [b) MasterWorker](#p5b)
- [**VI - Test de performance Master-Worker distribu√©e**](#p6)
<br><br><br>


## <a name="p1"></a> I - M√©thode de Monte-Carlo

La m√©thode de Monte-Carlo permet de pr√©dire un r√©sultat √† partir d'un ensemble de valeurs g√©n√©r√©es al√©atoirement.<br>
Elle effectue plusieurs it√©rations pour recalculer et affiner le r√©sultat.<br>
L'un des principes fondamentaux de la m√©thode est qu'une augmentation du nombre d'it√©rations am√©liore la pr√©cision de l'estimation.<br>
<br>
*M√©thode d'apr√®s IBM : https://www.ibm.com/fr-fr/topics/monte-carlo-simulation* <br>
<br><br>
**Calcul de œÄ par une m√©thode de MonteCarlo, d'apr√®s le TP4 fait en cours**<br>
Soit l'aire $`A_{\tfrac{1}{4}d}`$ d'un quart de disque de rayon $`r = 1`$.<br>

$$A_{\tfrac{1}{4}d} =  \frac{\pi r^2}{4} = \frac{\pi}{4}$$


<br><img src="img/figure1.png" width="300"/><br>
_**Figure 1 :** Tirage al√©atoire dans un carr√© de c√¥t√© r = 1._


Soit l'aire d'un carr√© de c√¥t√© $`r=1`$, $`A_{c} = r^2 = 1`$ <br>
<br>
Soient les points $`X_{p}(x_{p},y_{p})`$ dont les coordonn√©es sont tir√©es selon une loi $`U (]0,1[)`$.<br>
La probabilit√© que $`X_{p}`$ soit tir√© dans le quart de disque est

$$P = \frac{A_\frac{{1}{4}d}{A_{c}}} = \frac{\pi}{4}$$

On effectue $`n_{tot}`$ tirages al√©atoires.<br>
Soit $`n_{cible}`$ le nombre de points tir√©s dans le quart de disque.<br>
<br>
Si $`n_{tot}`$ est rand, alors on peut approximer P pour $`P = \frac{n_{cible}}{n_{tot}} \approx \frac{\pi}{4}`$<br>
<br>
D'o√π $`\pi \approx 4 \times \frac{n_{cible}}{n_{tot}}`$<br>
<br><br>

**ALGORITHME : Impl√©mentation s√©quentielle de la m√©thode de Monte-Carlo**<br>

<img src="img/algo_mc.png" width="500"/>


- On identifie deux
  - T0 : Tirer et compter $`n_{tot}`$ points
  - T1 : Calculer $`\pi`$
- T0 se d√©compose en $`n_{tot}`$ sous t√¢ches
  - $`T_{0P1}`$ : tirer $`X_{p}`$
  - $`T_{0P2}`$ : incr√©menter $`n_{cible}`$
- *D√©pendance entre t√¢ches :*
  - T1 d√©pend de T0
  - $`T_{0P2}`$ d√©pend de $`T_{0P1}`$
  - Les $`T0P1`$ sont ind√©pendats selon p
  - Les $`T0P2`$ sont ind√©pendats selon p


<br><br><br>
## <a name="p2"></a> II - Algorithme et parall√©lisation

### <a name="p2a"></a> a) It√©ration parall√®le

L'it√©ration parall√®le est aussi appel√©e parall√©lisme de boucle et parall√©lisme it√©ratif.<br>
Dans un algorithme parall√®le, on suppose que le calcul effectu√© par une unit√© de calcul est ind√©pendant de celui effectu√© par une autre unit√© de calcul.<br>
L'adjectif "parall√®le" attribu√© √† un tel algorithme provient du fait que cela correspond √† une architecture de type SIMD (une Seule Instruction, Plusieurs Donn√©es).<br>
*Source :*
  - *Explications adapt√©s pour Assigment102 :*<br>
    https://dpt-info.u-strasbg.fr/~cronse/TIDOC/ALGO/parseq.html


<br>
L'algorithme √† it√©ration parall√®le de la m√©thode de Monte-Carlo que nous avons √©tudi√© est Assignment102.<br>
<br>

Voici un extrait du code d'Assigment102 :<br>
```
	class MonteCarlo implements Runnable {
		@Override
		public void run() {
			double x = Math.random(); // g√©n√©ration d'un nombre al√©atoire, donc avec y => g√©n√©ration d'un point
			double y = Math.random(); // Si on pr√©cise une graine (seed) : le tirage est d√©terministe. Utile si on veut retester le code (donc en s'assurant qu'on obtient le m√™me r√©sultat)
			if (x * x + y * y <= 1)
				nAtomSuccess.incrementAndGet(); // On remarque que √ßa resemble √† notre pseudo code avec machin++. C'est un msg envoy√© √† nAtomSuccess
		}
	}

	public double getPi(int numWorkers) {
		int nProcessors = numWorkers; // runtime : c'est l'environnement pendant le temps d'ex√©cution du code. Ici elle nous propose de regarder le nb processeurs dispo
		ExecutorService executor = Executors.newWorkStealingPool(nProcessors); // il fixe le nb de thread au nb de processeur d√©tect√© par la JVM (donc si dans le bios on a autoris√© l'hyperthreading, alors il va dire que c'est 8 professeurs (8 coeurs en r√©alit√©s), sinon 4)
		for (int i = 1; i <= nThrows; i++) {
			Runnable worker = new MonteCarlo();
			executor.execute(worker);
		}
		executor.shutdown();
		while (!executor.isTerminated()) {
		}
		value = 4.0 * nAtomSuccess.get() / nThrows;
		return value;
	}
```

### <a name="p2b"></a> b) Master-Worker

Le premier algorithme Master-Worker de la m√©thode de Monte-Carlo que nous avons √©tudi√© est Pi.java.<br>
Master cr√©e puis lance des t√¢ches qui, via le paradigme des futures, font une r√©solution de d√©pendance.<br>
<br>
Pi.java est plus efficace que Assignment102, puisque si l'on cr√©e 500 000 t√¢ches, l'OS g√®re lui-m√™me celles-ci, ce qui prend plus de temps que de faire :
g√©n√©ration nombre al√©atoire x, g√©n√©ration nombre al√©atoire y, test, incr√©ment (~40 cycles).<br>
<br><br>
Il est difficile de montrer un exemple du code calculant Monte-Carlo, mais nous pouvons le visualiser via le code disponible sur ce d√©p√¥t.<br>
<br><br>


<br><br><br>

## <a name="p3"></a> III - Mise en ≈ìuvre sur machine √† m√©moire partag√©e

### <a name="p3a"></a> a) Analyse d'Assigment102

Le code de Assignment102 calcule une valeur approximative de ùúã √† partir de la m√©thode de Monte-Carlo.<br>
On y retrouve deux d√©pendances nouvelles : AtomicInteger et Executor.<br>

- **Atomic Integer :**<br>
  Encapsule une valeur enti√®re qui peut √™tre mise √† jour de mani√®re atomique.<br>
  C'est-√†-dire que toutes les op√©rations de lecture-modification-√©criture sur cette valeur sont effectu√©es comme une seule unit√© ins√©cable. Cela garantit la s√©curit√© des threads (thread-safety) sans n√©cessiter de synchronisation explicite.<br>
  Un AtomicInteger est utilis√© dans les applications telles que les compteurs incr√©ment√©s atomiquement et ne peut pas √™tre utilis√© comme remplacement d‚Äôun java.lang.Integer.<br>  <br>
  *Sources :*
  - *https://www.jmdoudoux.fr/java/dej/chap-acces_concurrents.htm*
  - *https://learn.microsoft.com/fr-fr/dotnet/api/java.util.concurrent.atomic.atomicinteger?view=net-android-34.0*
  - *ChatGPT (la phrase du "C'est-√†-dire")*
    <br><br><br>
- **Executor :**<br>
  L'interface java.util.concurrent.Executor d√©crit les fonctionnalit√©s permettant l'ex√©cution diff√©r√©e de t√¢ches impl√©ment√©es sous la forme de Runnable.<br>
  C‚Äôest un support pour les threads en Java √† un niveau plus √©lev√© que la classe Thread.<br>
  Elle permet de d√©coupler la soumission des t√¢ches de la gestion des threads (ex√©cution, ordonnancement).<br>
  On peut ici g√©rer des pools de threads. Chaque thread du pool peut √™tre r√©utilis√© dans un Executor.<br>
  L‚Äôinterface Executor d√©finit la m√©thode execute.<br>
  <br>
  *Sources :*
  - *https://jmdoudoux.developpez.com/cours/developpons/java/chap-executor.php#executor-1*
  - *Cours de M. DUFAUD : CM4-complement-parallelisation-et-Java.pdf*
- **Algorithme Workstealing (abord√© en cours) :**<br>
  Le principe de cet algorithme est que chaque worker poss√®de une liste de t√¢ches.<br>
  Lorsqu'un worker n'en poss√®de plus, plut√¥t que d'attendre de nouvelles t√¢ches, il va en extraire chez un autre worker.<br>
  <br>
  De cette mani√®re, non seulement chaque worker a √† peu pr√®s le m√™me nombre de t√¢ches, mais on s'assure √©galement que les workers restent toujours actifs.<br>
  On garantit donc l'optimisation du fonctionnement du programme.<br>*Sources :*
  - https://www.linkedin.com/advice/0/what-benefits-drawbacks-using-work-stealing?lang=fr&originalSubdomain=fr
- **Les futures :**<br>
  Le paradigme de programmation est tr√®s pratique car il permet de d√©finir, d√®s la cr√©ation de la t√¢che, le sch√©ma de d√©pendance entre les t√¢ches.<br>
  L'id√©e est de dire que la t√¢che va envoyer un r√©sultat dans le futur. On ne sait pas quand elle va l'envoyer, mais plus loin dans le code, on va utiliser ce r√©sultat.<br>
  On dit que la t√¢che x va envoyer un r√©sultat y dans le temps. La t√¢che s'ex√©cute, d'autres op√©rations s'ex√©cutent en parall√®le, puis √† un moment donn√©, x renvoie le r√©sultat et un calcul est effectu√©.<br>
  - √Ä ce moment-l√†, pour effectuer le calcul, le code doit attendre le r√©sultat de x, c'est un point de synchronisation.<br>
<br><br>
On d√©finit un objet executor qui est un support de thread.<br>
    On va lui dire : tu d√©finis le nombre de threads que tu vas utiliser.<br>
    -> Il permet de faire l'association entre des t√¢ches et des threads de mani√®re dynamique, en fonction de l'ex√©cution des t√¢ches en cours.<br>
    -> Il est donc possible d'associer 1000 t√¢ches √† 4 threads (au lieu de 1000 t√¢ches = 1000 threads dans le TP mobile, vu qu'on ne pouvait pas utiliser Executor).<br>
    <br>
    ===> On utilise d√©sormais la classe Executor, plut√¥t que la classe Thread.<br>
    Les threads en Java pr√©sentent de nombreux inconv√©nients, essentiellement li√©s au fait que cette classe est de bas niveau.<br>
    De ce fait, elle ne propose pas des fonctionnalit√©s telles que l'obtention d'un r√©sultat d'ex√©cution d'un thread ou encore l'attente de la fin d'un ensemble de threads.<br>
<br>
*Sources :*<br>
*https://jmdoudoux.developpez.com/cours/developpons/java/chap-executor.php#executor-1*

<br><br>
**Note :**
- start et run sont deux choses diff√©rentes
- V est un type g√©n√©rique
- overhead : ordonnanceur de l'OS

<br><br>
**Quelques annotations sur le code :**<br>
- **double y = Math.random();**
  - Si on pr√©cise une graine (seed) : le tirage est d√©terministe. Utile si on veut retester le code (donc en s'assurant qu'on obtient le m√™me r√©sultat)
- **int nProcessors = Runtime.getRuntime().availableProcessors();**
  - runtime : c'est l'environnement pendant le temps d'ex√©cution du code. Ici elle nous propose de regarder le nb processeurs dispo
- **Executors.newWorkStealingPool(nProcessors);**
  - il fixe le nb de thread au nb de processeur d√©tect√© par la JVM (donc si dans le bios on a autoris√© l'hyperthreading, alors il va dire que c'est 8 professeurs (8 coeurs en r√©alit√©s), sinon 4)
<br><br>

Voici un diagramme UML d'Assigment102 :<br>
<img src="img/uml_assigment102.png" width="600"/><br>

*Les outils suivants m'ont aid√© : StarUML, ChatGPT*<br>

### <a name="p3b"></a> b) Analyse Pi.java


**Paradigme de Pi.java :** Master Worker


Voici la version corrig√©e de ton texte :

On a un Master qui va cr√©er puis lancer des t√¢ches qui, via le paradigme des futures, effectuent une r√©solution de d√©pendance.<br>
<br><br>

Voici un sch√©ma montrant l'interraction entre 1 Master et 3 Workers : <br>
<img src="img\schema_dufaud_1M_3W_1M.png" width="600"/><br>

*Les outils suivants m'ont aid√© : Excalidraw*<br>


<br><br>
Pi.java est plus efficace que Assignment102, puisque si l'on cr√©e 500 000 t√¢ches, l'OS g√®re lui-m√™me celles-ci, ce qui prend plus de temps que de faire :
g√©n√©ration du nombre al√©atoire x, g√©n√©ration du nombre al√©atoire y, test, incr√©ment (~40 cycles).<br>
<br><br>
**Quelques annotations sur le code :**<br>
- **Executors.newFixedThreadPool(numWorkers)**
  - Renvoie une instance de type ExecutorService qui utilise un pool de threads dont la taille est fixe. Les t√¢ches √† ex√©cuter sont stock√©es dans une queue.
<br><br>

Voici un diagramme UML de Pi.java :<br>
<img src="img/uml_pi.java.png" width="600"/><br>

*Les outils suivants m'ont aid√© : StarUML, ChatGPT*<br>

<br><br>
Avec le mod√®le l'utilisation du paradigme Master-Worker, nous pouvons facilement r√©partir la partie Worker sur une autre machine avec quelque modification.<br>
De ce fait, Pi.java satisfait le crit√®re "flexible" du Context Coverage de la Quality in use.<br>

<br><br><br>

## <a name="p4"></a> IV - Qualit√© et test de performance (cf R05.08 Q Dev)


Dans cette partie et les parties qui suivent, nous utilisons la norme ISO_IEC_25010_2011.


### <a name="p4a"></a> a) Mise en place


Voici la version corrig√©e et am√©lior√©e de ton texte :

De prime abord, Assignment102 et Pi.java affichent les r√©sultats de mani√®re diff√©rente. Afin de mieux comparer et √©tudier leurs efficacit√©s, nous avons d√ª uniformiser les sorties.<br>
C'est-√†-dire s'assurer que chaque code affiche/renvoie la m√™me chose.<br>
<br><br>
Ainsi, les prints ci-dessous sont dans le m√™me format, que ce soit dans Assignment102 ou Pi.java.<br>

<img src="img\uni_sorties.png" width="1000"/><br>
Code de Pi.java
<br><br>
Pour traiter ces informations et faciliter l'automatisation du lancement du code plusieurs fois d'affil√©e, nous allons sauvegarder ces informations dans un document.<br>
Pour cela, nous allons cr√©er une classe WriteToFile :<br>
<img src="img\wtf.png" width="600"/><br>
<br><br>
Comme nous pouvons le voir, nous cr√©ons en premier un nom pour notre fichier.<br>
Nous indiquons le jour du test, ainsi que la machine sur laquelle nous testons (gr√¢ce √† InetAddress.getLocalHost()), sachant que le nom de chaque machine de l'IUT correspond √† la salle et √† sa position dans la salle.<br>
C'est indispensable car les r√©sultats peuvent varier d'une machine √† l'autre.<br>
Pour de meilleurs r√©sultats, dans les faits, il faudrait m√™me fermer tous les logiciels ouverts, voire m√™me utiliser le terminal plut√¥t qu'IntelliJ et, dans l'id√©al, ne pas utiliser l'interface graphique de Windows.<br>
<br>
Ensuite, nous √©crivons √† la suite du fichier les donn√©es re√ßues, nous sauvegardons et affichons si tout s'est bien pass√© ou non.<br>

<br><br>
Enfin, nous allons traiter ces fichiers via un programme Python afin d'√©tablir un graphique et √©tudier les r√©sultats.<br>
Plus pr√©cis√©ment, nous allons √©tudier la scalabilit√© forte et faible d'Assignment102 et Pi.java sur les crit√®res de temps d'ex√©cution et de speed-up.<br>

### <a name="p4b"></a> b) D√©finitions des termes
- **Speed-up :**<br>
  Le speed-up, not√© Sp, est le gain de vitesse d‚Äôex√©cution en fonction du nombre de processus P.<br>
  L'id√©e est donc de mesurer le gain de performance obtenu en ex√©cutant une t√¢che sur plusieurs processeurs (ou c≈ìurs) par rapport √† un seul processeur.<br>
  <br>
  Il se calcul de cette mani√®re : $`S_p = \frac{T_1}{T_p}`$ : <br>
  Avec :
  - $`S_p`$ le speedup 
  - $`T_1`$ le temps d‚Äô√©x√©cution sur un processus
  - $`T_P`$ le temps d‚Äô√©x√©cution sur P processus
- **Temps d'ex√©cution**<br>
  Le temps d'ex√©cution correspond au temps que demande le programme pour effectuer le calcul de œÄ.<br>
  Dans le cas du paradigme Master-Worker, le temps d'ex√©cution correspond au temps des √©changes entre le Master et les Workers, au temps que demandent les Workers pour calculer, ainsi qu'au temps n√©cessaire pour assembler le r√©sultat final par le Master.
- **Scalabilit√© forte :**<br>
  La scalabilit√© forte consiste √† √©tudier ce qu'il se passe lorsque l'on ajoute des processus pour un probl√®me de taille fixe.
- **Scalabilit√© faible :**<br>
  La scalabilit√© faible consiste √† √©tudier ce qu'il se passe lorsqu'on augmente simultan√©ment la taille du probl√®me et le nombre de processus.



### <a name="p4c"></a> c) Etude sur le crit√®re d'efficiency
L'objectif de cette √©tude est de prouver quel est le meilleur paradigme pour calculer œÄ √† l'aide de la m√©thode de Monte-Carlo, entre Assignment102 et Pi.java.<br>
Nous √©tudons donc l'efficiency des programmes : quel est le programme le plus efficace en terme de temps, d'utilisation des ressources et de marge d'erreur.<br>
<br>
Pour rappel :
- Paradigme d'Assignment102 : It√©ration parall√®le
- Paradigme de Pi.java : Master-Worker

<br><br>

- Les tests suivant ont √©t√© effectu√©s sur un des ordinateurs de la rang√©e de droite de la salle G24, voici sa configuration :
  - Processeur : Intel Core i7-9700 3.00GHz - 8 coeurs - DDR4-2666
    - https://www.intel.fr/content/www/fr/fr/products/sku/191792/intel-core-i79700-processor-12m-cache-up-to-4-70-ghz/specifications.html
  - RAM : 32Go
  - Windows 11 Pro 23H2, build : 22631.4169
  - Attention, certains logiciel fonctionnaient en fond, par ailleurs l'interface graphique de Windows √©tait d√©marr√©. Cela influe sur les performances de l'ordinateur.



- Information :
  - **Programme utilis√© :** analyseur/analyseur_scalabilit√©s.py
  - **Fichier √©tudi√© pour Pi.java :** 17-12-2024_121434_Pi-java_G24-5_output.txt
  - **Fichier √©tudi√© pour Assignment102 :** 16-12-2024_112105_Assigment102_G24-5_output.txt


**R√©sultats obtenus :**
<img src="img\etude_sca.png"><br>

- **Exp√©rience n¬∞1 en Scalabilit√© Forte : Est-ce que le temps d‚Äôex√©cution diminue lorsque j‚Äôajoute des processus pour un probl√®me de taille fixe ?**
  <img src="img\etude_sca_e1.png"><br>
  Nous remarquons que contrairement √† Assignment102, pour Pi.java, le temps d'ex√©cution diminue.<br>
  Cela s'explique par le fait que l'on divise la taille du probl√®me entre les processus. Vu que les processus traite des probl√®mes de plus en plus petit, c'est tr√®s rapide bien qu'ils soient nombreux.<br>
  En revanche, pour Assignment102, nous donnons un probl√®me de m√™me taille √† chaque processus, donc √ßa ne peut qu'augmenter. Cependant nous remarquons que √ßa augmente de plus en plus lentement. <br>
  <br>
  Pi.java est donc le gagnant de cette exp√©rience.


- **Exp√©rience n¬∞2 en Scalabilit√© Forte : Comment les ressources sont utilis√©s lorsque j'ajoute des processus pour un probl√®me de taille fixe ?**
  <img src="img\etude_sca_e2.png"><br>

  La courbe bleu correspond au speed-up id√©al en fonction du nombre de processus.<br>
  Cela signifie que dans l'id√©al, si la m√©thode est parfaitement parall√®le, le fait de doubler le nombre de processus divise par deux le temps de calcul. En d'autre terme : il y a une excellente r√©partiion des ressources.<br>
  La courbe du speedup id√©al augmente rapidement puisque chaque processus est cens√© s'ex√©cuter plus rapidement √† chaque ajout (vu que le totalCount attribu√© diminue √† chaque ajout).<br>
  Si le point de notre de mesure est au dessus du speedup id√©al, c'est que le processus r√©sout plus rapidement le probl√®me que dans la logique, sinon si c'est en dessous, c'est qu'il met plus de temps.<br>
  <br>
  - **Pi.java**<br>
    Nous remarquons que le speedup suit le speed-up id√©al au d√©but lorsque l'on utilise 1 √† 8 processus, puis augmente tr√®s lentement de 8 √† 16 processus avant de stagner.<br>
    Cela signifie que Pi.java est efficace lorsque l'on utilise un petit nombre de processus.<br>
    Apr√®s cela, on peut d√©duire que l'interaction de plus en plus importante entre le Master et les Workers fait perdre de l'efficacit√© au programme. Ce temps de traitement de plusieurs processus par la machine s'appel "overhead", nous en parlons en partie V a).
  - **Assignment102**<br>
    C'est la catastrophe ! M√™me avec plus de processus, le r√©sultat est mauvais : cela tend vers 0.<br>
    Cela ressemble √† une courbe de scalabilit√© faible (voir exp√©rience n¬∞4), mais c'est bien le r√©sultat de notre exp√©rience en scalabil√© forte.<br>
    On en d√©duit une tr√®s mauvaise r√©partition de la charge de travail entre les processus, chose que nous avons d√©j√† remarqu√© dans l'exp√©rience n¬∞1.
  
  <br>
  Par cons√©quent, Pi.java est de nouveau vainqueur : cet algorithme est plus efficace qu'Assigment102.

- **Exp√©rience n¬∞3 en Scalabilit√© Faible : Est-ce que le temps d‚Äôex√©cution diminue lorsque j‚Äôaugmente simultan√©ment la taille du probl√®me et le nombre de processus ?**
  <img src="img\etude_sca_e3.png"><br>
  Nous remarquons que dans les deux cas, le temps d'ex√©cution augmente. Cependant, ici √©galement Pi.java reste le plus effiace : il augmente beaucoup moins vite qu'Assignment102 si l'on se fie √† l'axe des ordonn√©es.<br>
  Nous remarquons √©galement que c'est une courbe lin√©aire, ce qui est normal puisque le probl√®me grandi proportionnellement au nombre de processus.<br>
  Il est a not√© que dans le cas de Pi.java, de 1 √† 8 processus le temps est constant, √ßa augmente d√®s qu'il y a plus de 8 processus.
  <br>
  M√™me lors d'une √©tude de scalabit√© faible, Pi.java est mieux qu'Assignment102.


- **Exp√©rience n¬∞4 en Scalabilit√© Faible : Comment les ressources sont utilis√©s lorsque j‚Äôaugmente simultan√©ment la taille du probl√®me et le nombre de processus ?**
  <img src="img\etude_sca_e4.png"><br>
  Ici, le speedup id√©al est constant. Bien qu'on augmente le totalCount g√©n√©ral, chaque processus calcul le m√™me totalCount et donc par cons√©quent, prend le m√™me temps d'ex√©cution dans l'id√©al.<br>
  <br>
  Notre observation rejoins l'oberservation sur l'exp√©rience n¬∞2 : augmenter le nombre de processus ne permet pas toujours d'avoir un bon speed-up, m√™me si la taille du probl√®me augmente proportionnellement.<br>
  En effet, comme dans l'exp√©rience n¬∞2, de 1 √† 8 la courbe suit le speed-up id√©al, apr√®s cela le co√ªt des communications entre le Master et les Workers influe trop sur les performances.<br>
  De m√™me que dans l'exp√©rience n¬∞2, Assignment102 s'effond rapidement.<br>
  <br>
  Seulement, je me demande si Pi.java ne finira pas par stagner comme Assignment102 lorsque l'on compare l'allure des deux courbes.
  <br>
  La conlusion reste l√† m√™men Pi.java est mieux qu'Assignment102.<br>


A l'issue de ces quatre exp√©rience, la conclusion est sans apppel : le paradigme Master-Worker offre des meilleurs r√©sultats avec la m√©thode de Monte-Carlo que le paradigme d'it√©ration parall√®le.<br>
A partir de l√†, nous pouvons √©tudier les taux d'erreur de Pi.java sous la forme d'une exp√©rience n¬∞5.<br>

- **Exp√©rience n¬∞5 : Taux d'erreurs par rapport au nombre de points calcul√©s par Pi.java**
  <img src="img\etude_sca_e5.png"><br>
  <br>
  L√©gende :
  - **Points rouges :** Repr√©sente le taux d'erreur "Error" pour chaque calcul, c'est-√†-dire chaque ligne du fichier pi.txt
  - **Points noir :** Repr√©sente la m√©diane des taux d'erreurs 
  - **ordonn√©e :** le taux d'erreur
  - **abscisse :** Le nombre de processus Nproc en millions (donc 1M = 1000000)
  <br><br>

  Information :
  - **Programme utilis√© :** analyseur/analyseur_erreurs.py
  - **Fichier √©tudi√© :** 16-12-2024_215711_Pi-java_DESKTOP-9GESL6B_output.txt
  - **Fichier √©tudi√© partie Scalabilit√© Forte :** 16-12-2024_215711_Pi-java_DESKTOP-9GESL6B_output__pi_scalabilit√©_forte.txt
  - **Fichier √©tudi√© partie Scalabilit√© Faible :** 16-12-2024_215711_Pi-java_DESKTOP-9GESL6B_output__pi_scalabilit√©_faible.txt


  Nous remarquons que lors de notre √©tude de la scalabilit√© faible, plus il y avait de points totaux (totalCount), plus l'algorithme est fiable.<br>
  Il est a not√© que chaque processus calcul "totalCount / nbProcessus", cela signifie dans notre cas que peu importe le nombre de processus, chaque processus calcul Pi pour totalCount = 1000000.<br>
  Seulement, √† la fin, cela fait bien 1M * nbProcessus, ce que nous repr√©sentons en abscisse dans le graphique.<br>
  <br>
  Il faut bien comprendre qu'on r√©parti le totalCount d'origine entre les processus en amont de l'attribution de la valeur au Worker lors de sa cr√©ation. Sinon, le calcul de pi est faux (on aurait fait deux fois une division par nbProcessus).<br>

<br><br><br>


### <a name="p4d"></a> d) Etude sur le crit√®re d'effictiveness pour Pi.java

M√™me si Pi.java est performant et que sont taux d'erreur diminue plus on augmente le totalCount... Quand est-il de son efficitiveness ?
Nous allons donc √©tudier l'effictiveness de Pi.java, c'est-√†-dire sa capacit√© √† r√©soudre le probl√®me parfaitement sur le crit√®re suivant :<br>
<br>
**Est ce que Pi.java permet de calculer le nombre Pi sur $`10^{-3}`$ ?**
<br>
La valeur de Pi pour $`10^{-3}`$ est : **3,141**<br>
Nous allons donc v√©rifier si lors de nos calculs, Pi.java renvoie bien cette valeur. Nous tiendrons uniquement compte des 3 premiers chiffres apr√®s la virgule.<br>
<br>
Le tableau ci-dessous correspond au bilan de la v√©rification depuis le fichier pi.txt.<br>
Dans la colonne "OK", nous indiquons le nombre de fois o√π nous obtenons 3,141 comme valeur. Il est de m√™me pour "HS", dans le cas o√π le r√©sultat est invalide.<br>
<br>
**Scalablit√© Forte**

| Nlance | Nproc | OK    | HS    |
|--------|-------|-------|-------|
| 15M    | 1     | 19/25 | 06/25 |
| 15M    | 2     | 19/25 | 06/25 |
| 15M    | 4     | 17/25 | 08/25 |
| 15M    | 8     | 18/25 | 07/25 |
| 15M    | 16    | 18/25 | 07/25 |
| 15M    | 32    | 17/25 | 08/25 |
| 15M    | 64    | 18/25 | 07/25 |

**Scalablit√© faible**

| Nlance | Nproc | OK    | HS    |
|--------|-------|-------|-------|
| 1M     | 1     | 8/25  | 17/25 |
| 2M     | 2     | 12/25 | 13/25 |
| 4M     | 4     | 14/25 | 11/25 |
| 8M     | 8     | 17/25 | 08/25 |
| 16M    | 16    | 22/25 | 03/25 |
| 32M    | 32    | 24/25 | 01/25 |
| 64M    | 64    | 25/25 | 00/25 |


<br><br>
Dans le cas de la scalablit√© forte, majoritairement l'algorithme trouve 3,141. On est √† 6 √† 8 rat√©s pour 25 tentatives au total soit un taux de succ√®s de 72% en moyenne. L'ajout de processus suppl√©mentaire n'am√©liore pas le r√©sultat<br>
Dans le cas de la scalabilit√© faible en revanche, plus il y a de processus mieux nous trouvons 3,141 ! Le r√©sultat est catastrophique pour Nproc = 1 et excellent pour Nproc = 64.<br>
Cela correspond √† ce qu'on a vu comme r√©sultat √† l'exp√©rience n¬∞5.<br>
<br>
En conclusion, pour calculer 3,141 l'effictiveness de Pi.java est excellente √† partir de Nproc = 64 pour totalCount = 64M dans le cas de la scalabilit√© faible.<br>
Il vaut donc mieux utiliser Pi.java en scalabilit√© faible qu'en scalabit√© forte.<br>


### <a name="p4e"></a> e) Etude sur le crit√®re de satisfaction pour Pi.java
Maintenant, nous allons v√©rifier que Pi.java correspond bien √† nos attentes en tant que client.<br>
<br>
- **Usefulness :** est ce que le programme est utile ?<br>
  Dans notre cas, le programme est utile pour faire notre √©tude.<br>
  En g√©n√©ral, l'algorithme de Monte-Carlo utilis√© pour mod√©liser des syst√®mes incertains ou al√©atoires dans divers domaines tel que la Finance, la Physique statique et la biologie (selon ChatGPT).


- **Trust :** est ce que le programme respecte la norme ?<br>
  Pour assurer notre confiance envers le programme, nous devons v√©rifier que le g√©n√©rateur de nombre al√©atoire g√©n√®re bien des nombres al√©atoires ind√©pendant entre chaque processus.<br>
  <br>
  Le g√©n√©rateur de nombre al√©atoire se trouve dans la boucle for de la m√©thode `call` de la classe `Worker`.<br>
  Nous allons donc √©crire dans un fichier de texte les valeurs enregistr√©s pour quel Worker (son id) et pour quel tour (la variable j). Non indiquerons dans un nouveau attribu "id" de la classe, le num√©ro du Worker.<br>
  Afin de distinguer les ex√©cutions du programme, nous indiquerons juste avant les crit√®res de lancement tel que le nombre de worker ou le totalCount.<br>
  <br>
  L'exp√©rience qui suit a √©t√© fait sur mon ordinateur avec la configuration suivante :

  - Fichier : Pi.java
    - En : Scalabilit√© forte
    - Nombre de processus : 1, 2, 4, 8
    - totalCount : 15000000
  - PC :
    - Processeur : Intel Core i5-9400F, 2.90Ghz, 6 Coeurs, 1 Socket
    - RAM : 8Go
    - Windows 10 Home 22H2, build : 19045.5247
    - Attention, certains logiciel fonctionnaient en fond, par ailleurs l'interface graphique de Windows √©tait d√©marr√©. Cela influe sur les performances de l'ordinateur.
    
  <br><br>
  Voici une s√©lection du fichier `17-12-2024_Pi-java_trust_DESKTOP-9GESL6B_output.txt`. Notre analyse se fondera dessus :
  ```
  =========== tour n¬∞4 Nworker = 2 totalCount = 7500000 -- heure : 205745
    --[W0 - tour 0]-->(0.6081051753721731, 0.6047682773649212)
    --[W1 - tour 0]-->(0.7278271711041955, 0.6775989547714912)
    --[W0 - tour 1]-->(0.20780501411925578, 0.8716607124040157)
    --[W1 - tour 1]-->(0.5941601270829872, 0.061907042047551264)
    --[W0 - tour 2]-->(0.05355968858713944, 0.7126668019026495)
    --[W1 - tour 2]-->(0.887138018183691, 0.20071311431511185)
    --[W0 - tour 3]-->(0.24020044011396913, 0.07554104600422717)
    --[W1 - tour 3]-->(0.3866213958047845, 0.5359512604793124)
    --[W0 - tour 4]-->(0.3227026009876759, 0.37004047777817484)
    --[W1 - tour 4]-->(0.45311327236889853, 0.2326071153151319)

  =========== tour n¬∞0 Nworker = 4 totalCount = 3750000 -- heure : 205745
    --[W0 - tour 0]-->(0.8025528982424484, 0.2226773653722971)
    --[W1 - tour 0]-->(0.3248899870176598, 0.04926276744406999)
    --[W2 - tour 0]-->(0.5225164923121788, 0.6900527962622921)
    --[W3 - tour 0]-->(0.43812046529937565, 0.07867120537337235)
    --[W1 - tour 1]-->(0.7862151186163597, 0.36044700594397694)
    --[W0 - tour 1]-->(0.4851773188977315, 0.6621674785597783)
    --[W2 - tour 1]-->(0.006746046782472925, 0.6354973634446587)
    --[W3 - tour 1]-->(0.4077618482875708, 0.6449175124991942)
    --[W1 - tour 2]-->(0.04196849409072456, 0.088908523700715)
    --[W0 - tour 2]-->(0.2926212471609845, 0.7998970075375)
    --[W2 - tour 2]-->(0.63435248239826, 0.8108491937973279)
    --[W3 - tour 2]-->(0.8324205636569904, 0.47053779689037745)
    --[W1 - tour 3]-->(0.2730817545547871, 0.2964243338384539)
    --[W0 - tour 3]-->(0.5155221502509648, 0.8220765104699722)
    --[W2 - tour 3]-->(0.6719261799865358, 0.963865894251568)
    --[W3 - tour 3]-->(0.12983509879140642, 0.7102248715438713)
    --[W1 - tour 4]-->(0.461360743383905, 0.9939536120283735)
    --[W0 - tour 4]-->(0.9480406778244539, 0.4022095903943069)
    --[W2 - tour 4]-->(0.9439628526362639, 0.9553383219811445)
    --[W3 - tour 4]-->(0.38479681908071495, 0.16050873876455507)
  ```

  Ces deux appels au programme se diff√©rencie par le nombre de Worker et le totalCount qui a augment√© entre temps.<br>
  Ces donn√©es sont involontairement class√© par l'it√©ration de la variable `j` dans la boucle for. Cela simplifie nos comparaisons.<br>
  <br>
  Nous remarquons que sur un m√™me tour, aucun Worker ne poss√®de le m√™me nombre g√©n√©r√© al√©atoire, que ce soit pour `x` (√† gauche) ou `y` (√† droite), qui sont les deux variables dans la boucle.<br>
  <br>
  Nous pouvons alors conclure que **oui, Pi.java est fiable puisque son g√©n√©rateur de nombre al√©atoire g√©n√®re bien des nombres al√©atoires distinct entre les Workers**.<br>
  Autrement, chaque Worker aurait fait le m√™me calcul, donc on aurait fait nWorker fois la m√™me chose m√™me si nous sommes all√© plus vite. Ca n'aurait donc servi √† rien.<br>
  La confiance "trust" d√©pend de la pr√©cision et de la qualit√© du code.


- **Pleasure :** est ce que on a des d√©sagr√©ment dans l'utilisation, est ce qu'on a de mauvaises surprise ?<br>
  Je suppose que oui, disons que le code s'ex√©cute rapidement et qu'il utilise assez bien la puissance de l'ordinateur. Il est facile d'utilisation je trouve.<br>


- **Comfort :**<br>
  Je ne sais pas.

## <a name="p5"></a> V - Mise en ≈ìuvre en m√©moire distribu√©e

### <a name="p5a"></a> a) (Analyse) JavaSocket

En Software, le **socket** est un fichier contenant des informations.<br>
Toutes nos donn√©es, toutes les cases m√©mores sont mis dans un fichier qui est transmis dans le r√©seau.<br>
<br>
Nous en venons au terme **overhead** qui est le temps de traitement de plusieurs processus par la machine.<br>
On peut le diminuer en augmentant la charge d'un processus, car traiter quelques gros processus est moins couteux que de g√©rer plusieurs petit processus.<br>
C'est pour cela que l'architecture d'une carte graphique, utile pour des calcules parall√®les n'est pas adapt√© pour faire fonctionner un syst√®me d'exploitation.<br>
Une carte graphique g√®re plusieurs petits processus contrairement √† un processeurs qui en g√®re principalement des gros. Avec un GPU, le syst√®me serait donc tr√®s lent !<br>

### <a name="p5b"></a> b) MasterWorker

Pour le dernier projet, on utilise deux fichiers java :

- Master, qui g√®re les Workers lanc√©s sur diff√©rents ports
- Worker, qui se lance sur un port pr√©cis. Ce qui permet d'en lancer plusieurs simultan√©ments sur diff√©rents ports.<br>
  <br><br>
  Notre objectif √† terme est de lancer un Master sur un ordinateur, et utiliser les Workers qui sont sur d'autres ordinateurs.<br>
  En cons√©quence, le code est pr√©vu pour : apr√®s une petite modification on peut donner des ip personnalis√©s √† Master pour acc√©der aux autres ordinateurs.<br>
  Pour le moment, nous allons tester sur une seule machine.<br>

Voici un diagramme UML complet de Master-Worker :<br>
<img src="img/uml_mw.png" width="800"/><br>

*Les outils suivants m'ont aid√© : StarUML, ChatGPT*<br>

<br><br>

J'ai du rajouter un morceau de code pour ex√©cuter la m√©thode de Monte-Carlo ainsi que l'enregistrement dans un fichier texte comme pour Assigment102 et Pi.java.<br>
On reprend simplement le morceau de code d'Assigment102 permettant de calculer Pi que l'on place dans une fonction qu'on appel.<br>
On aurait pu directement l'int√©grer au code, seulement celui-ci serait moins maintenable, il serait plus dure de retrouver puis de remplacer le morceau.<br>
Ici on sait ce qu'il a pr√©cis√©ment besoin et ce qu'il doit renvoyer, faire.<br>
<br>
<img src="img\permettraDeCalculerPi.png" width="350"/><br>
<img src="img\whileWorker.png" width="900"processus/>

<br>**Pour lancer et changer de port :**
- **Lancer sur le port 25545 :**<br>
  Sur IntelliJ pour WorkerSocker :
  - Cliquer sur le bouton "3 points vertical", puis "Edit" et mettre "25545" dans le port (sous distributedMC).
- **Lancer plusieurs instances :**
  Sur IntelliJ pour WorkerSocker :
  - Cliquer sur le bouton "3 points vertical", puis dans "Config", dans "Modify options (Build and Run)", cocher Allow m... (CTRL+U) en haut de la liste.
<br><br>
Si nous ex√©cutons nos programmes sur une seule machine, la r√©partition des Worker + Master correspond au sch√©ma du III - b).<br>
Si nous l'ex√©cutons sur plusieurs machine, cela correspondrais √† √ßa :<br>
<img src="img\schema_dufaud_1M_3W_1M_sur_plusieurs_pc.png" width="600"/><br>

## <a name="p6"></a> VI - Test de performance Master-Worker distribu√©e

Les deux exp√©riences suivante ont √©t√© men√©s en salle G24 le Vendredi 13 D√©cembre 2024.<br>
12 ordinateurs √† droite ont √©t√© utilis√©s en tant que Worker, 1 ordinateur √† gauche a √©t√© utilis√© en tant que Master.<br>
<br>

**Attention :** les ordinateurs "Workers" n'ex√©cutais pas tous Pi.java, certains √©taient sous Assignment102 !<br>
Seulement, nous ne tiendrons pas compte de l'impact que peut avoir Assignment102 dans notre analyse.<br>

- **Exp√©rience n¬∞6 en Scalabilit√© Forte : Comment les ressources sont utilis√©s lorsque j'ajoute des processus pour un probl√®me de taille fixe ?**<br>
  <img src="img\scalabilite_forte_MW_sur_machine_192000000.png"><bR><br>
L'exp√©rience n¬∞6 reprend la consigne de l'exp√©rience n¬∞2. M√™me si ici le totalCount est diff√©rent (192000000) et que la configuration est diff√©rente (utilisation d'un parc de 13 PC sous CentOS), nous remarquons une diff√©rence frappante entre ces deux exp√©riences !<br>
D√©j√†, le speed-up calcul√© suit le speed-up id√©al jusqu'√† 16 coeurs (16 processus) contrairement √† l'exp√©rience n¬∞2 o√π c'√©tait de de 1 √† 4.<br>
La courbe commence √† d√©vier √† partir de 16 coeurs mais reste proche de l'id√©al, il faudrait tester avec davantages de processus pour observer quand √ßa stagne. Dans l'exp√©rience n¬∞2, √ßa stagne d√®s 8 processus.<br>
Nous remarquons tout de suite la puissance d'un parc informatique assembl√© pour faire un calcul distribu√© plut√¥t qu'un seul pc pour un calcul parall√®le.<br>
<br>
Il est int√©ressant de noter que pour l'exp√©rience n¬∞2 nous sommes all√© jusqu'√† 64 processus, ici, nous nous arr√™tons √† 48. Mais nous pouvons assez bien imagin√© la suite des courbes et surtout nous pouvons compar√©s les r√©sultats ais√©ment.


- **Exp√©rience n¬∞7 en Scalabilit√© Faible : Comment les ressources sont utilis√©s lorsque j'ajoute des processus pour un probl√®me de taille fixe ?**<br>
  <img src="img\scalabilite_faible_MW_sur_machine_4000000.png"><br><br>
  Cette fois ci, c'est l'exp√©rience n¬∞4 qui est compar√©. La conclusion reste la m√™me, le r√©sultat du calcul distribu√© √©crase le calcul parall√®le.